{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pomysłem na rozwiązanie problemu rekomendacji jest stworzenie sieci neuronowej, która oceni filmy zgodnie z preferencjami danego użytkownika.\n",
    "\n",
    " W tym celu planuję przygotować dane, aby wektor wejściowy do sieci zawierał zarówno reprezentację ocenianego filmu, jak i reprezentację użytkownika. Posiadam jedynie dane dotyczące preferencji użytkowników, zatem będzie to jedyny wyznacznik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "movie=pd.read_csv(r\"ml-25m\\movies.csv\")\n",
    "rat=pd.read_csv(r\"ml-25m\\ratings.csv\")\n",
    "tag=pd.read_csv(r\"ml-25m\\tags.csv\")\n",
    "gs=pd.read_csv(r\"ml-25m\\genome-scores.csv\")\n",
    "gt=pd.read_csv(r\"ml-25m\\genome-tags.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zaczniemy od reorganizacji danych z tabeli genome_scores.**\n",
    "\n",
    "Dostaracza ona dużo informacji na temat treści filmu. Filmy są tam ocenione pod względem zawartości ponad 1000 tagów. Są podane jednak w oszczędnej dla pamięci formie, więc zreorganizuję tabele aby dostoswać ją pod wygodny wektor danych.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>1119</th>\n",
       "      <th>1120</th>\n",
       "      <th>1121</th>\n",
       "      <th>1122</th>\n",
       "      <th>1123</th>\n",
       "      <th>1124</th>\n",
       "      <th>1125</th>\n",
       "      <th>1126</th>\n",
       "      <th>1127</th>\n",
       "      <th>1128</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02875</td>\n",
       "      <td>0.02375</td>\n",
       "      <td>0.06250</td>\n",
       "      <td>0.07575</td>\n",
       "      <td>0.14075</td>\n",
       "      <td>0.14675</td>\n",
       "      <td>0.06350</td>\n",
       "      <td>0.20375</td>\n",
       "      <td>0.2020</td>\n",
       "      <td>0.03075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04050</td>\n",
       "      <td>0.01425</td>\n",
       "      <td>0.03050</td>\n",
       "      <td>0.03500</td>\n",
       "      <td>0.14125</td>\n",
       "      <td>0.05775</td>\n",
       "      <td>0.03900</td>\n",
       "      <td>0.02975</td>\n",
       "      <td>0.08475</td>\n",
       "      <td>0.02200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.04125</td>\n",
       "      <td>0.04050</td>\n",
       "      <td>0.06275</td>\n",
       "      <td>0.08275</td>\n",
       "      <td>0.09100</td>\n",
       "      <td>0.06125</td>\n",
       "      <td>0.06925</td>\n",
       "      <td>0.09600</td>\n",
       "      <td>0.0765</td>\n",
       "      <td>0.05250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05250</td>\n",
       "      <td>0.01575</td>\n",
       "      <td>0.01250</td>\n",
       "      <td>0.02000</td>\n",
       "      <td>0.12225</td>\n",
       "      <td>0.03275</td>\n",
       "      <td>0.02100</td>\n",
       "      <td>0.01100</td>\n",
       "      <td>0.10525</td>\n",
       "      <td>0.01975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.04675</td>\n",
       "      <td>0.05550</td>\n",
       "      <td>0.02925</td>\n",
       "      <td>0.08700</td>\n",
       "      <td>0.04750</td>\n",
       "      <td>0.04775</td>\n",
       "      <td>0.04600</td>\n",
       "      <td>0.14275</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.03875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06275</td>\n",
       "      <td>0.01950</td>\n",
       "      <td>0.02225</td>\n",
       "      <td>0.02300</td>\n",
       "      <td>0.12200</td>\n",
       "      <td>0.03475</td>\n",
       "      <td>0.01700</td>\n",
       "      <td>0.01800</td>\n",
       "      <td>0.09100</td>\n",
       "      <td>0.01775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.03425</td>\n",
       "      <td>0.03800</td>\n",
       "      <td>0.04050</td>\n",
       "      <td>0.03100</td>\n",
       "      <td>0.06500</td>\n",
       "      <td>0.03575</td>\n",
       "      <td>0.02900</td>\n",
       "      <td>0.08650</td>\n",
       "      <td>0.0320</td>\n",
       "      <td>0.03150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05325</td>\n",
       "      <td>0.02800</td>\n",
       "      <td>0.01675</td>\n",
       "      <td>0.03875</td>\n",
       "      <td>0.18200</td>\n",
       "      <td>0.07050</td>\n",
       "      <td>0.01625</td>\n",
       "      <td>0.01425</td>\n",
       "      <td>0.08850</td>\n",
       "      <td>0.01500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.04300</td>\n",
       "      <td>0.05325</td>\n",
       "      <td>0.03800</td>\n",
       "      <td>0.04100</td>\n",
       "      <td>0.05400</td>\n",
       "      <td>0.06725</td>\n",
       "      <td>0.02775</td>\n",
       "      <td>0.07650</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.02975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05350</td>\n",
       "      <td>0.02050</td>\n",
       "      <td>0.01425</td>\n",
       "      <td>0.02550</td>\n",
       "      <td>0.19225</td>\n",
       "      <td>0.02675</td>\n",
       "      <td>0.01625</td>\n",
       "      <td>0.01300</td>\n",
       "      <td>0.08700</td>\n",
       "      <td>0.01600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1        2        3        4        5        6        7     \\\n",
       "movieId                                                                  \n",
       "1        0.02875  0.02375  0.06250  0.07575  0.14075  0.14675  0.06350   \n",
       "2        0.04125  0.04050  0.06275  0.08275  0.09100  0.06125  0.06925   \n",
       "3        0.04675  0.05550  0.02925  0.08700  0.04750  0.04775  0.04600   \n",
       "4        0.03425  0.03800  0.04050  0.03100  0.06500  0.03575  0.02900   \n",
       "5        0.04300  0.05325  0.03800  0.04100  0.05400  0.06725  0.02775   \n",
       "\n",
       "            8       9        10    ...     1119     1120     1121     1122  \\\n",
       "movieId                            ...                                       \n",
       "1        0.20375  0.2020  0.03075  ...  0.04050  0.01425  0.03050  0.03500   \n",
       "2        0.09600  0.0765  0.05250  ...  0.05250  0.01575  0.01250  0.02000   \n",
       "3        0.14275  0.0285  0.03875  ...  0.06275  0.01950  0.02225  0.02300   \n",
       "4        0.08650  0.0320  0.03150  ...  0.05325  0.02800  0.01675  0.03875   \n",
       "5        0.07650  0.0215  0.02975  ...  0.05350  0.02050  0.01425  0.02550   \n",
       "\n",
       "            1123     1124     1125     1126     1127     1128  \n",
       "movieId                                                        \n",
       "1        0.14125  0.05775  0.03900  0.02975  0.08475  0.02200  \n",
       "2        0.12225  0.03275  0.02100  0.01100  0.10525  0.01975  \n",
       "3        0.12200  0.03475  0.01700  0.01800  0.09100  0.01775  \n",
       "4        0.18200  0.07050  0.01625  0.01425  0.08850  0.01500  \n",
       "5        0.19225  0.02675  0.01625  0.01300  0.08700  0.01600  \n",
       "\n",
       "[5 rows x 1128 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = gs.pivot(index='movieId', columns='tagId', values='relevance')\n",
    "data.columns.name = None\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kolejnym celem jest utworzenie reprezentacji uzytkownika**\n",
    "\n",
    "Postanowiłem zrobić to poprzez wektor jego ulubionych filmów, jako ze każdy film jest okreslony przez ponad 1000 cech postanowilem zmiejszyć ich liczbę i uszeregowac je wzgledem ważności i wybrać tylko te najwazniejsze do wektora jego preferencji. Chciałem aby sieć przyjmowała na wejsciu wektor reprezentujący użytkownika złożony z pewnej liczby jego ulubionych filmów które będą reprezentowane przez x najważniejszych cech, oraz wektor filmu do oceny stworzony w podobny sposób"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "n_components = 600\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(data)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "print(cumulative_explained_variance)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, n_components + 1), cumulative_explained_variance, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance vs. Number of Components')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać wyzej przy 600 cechach, zachowujemy prawie 96% informacji,\n",
    "jednak wystarczy jedynie 25 cech aby zachować około 60% informacji.\n",
    "Postanowiłem że wektory wejściowe będą złożone z 50 cech filmu ocenianego i 25 filmów należących do ulubionych uzytkownika."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14905495 0.22326949 0.28168725 0.32124008 0.35382191 0.38187087\n",
      " 0.40660537 0.4287136  0.44887958 0.46572842 0.48076429 0.49415438\n",
      " 0.50713443 0.51925706 0.53084176 0.54110816 0.55019388 0.5590928\n",
      " 0.56708517 0.57490315 0.58210175 0.58900122 0.5957148  0.6020578\n",
      " 0.60809961 0.61370185 0.61891792 0.62385818 0.62871808 0.63337181\n",
      " 0.6379532  0.64222105 0.64622715 0.65003532 0.65363208 0.65719773\n",
      " 0.66063974 0.66397997 0.66722317 0.67033736 0.67334321 0.67630123\n",
      " 0.67919024 0.6820058  0.68475618 0.68749435 0.69012061 0.69262602\n",
      " 0.69507328 0.69742407]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "n_components = 50\n",
    "\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(data)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "print(cumulative_explained_variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, explained_variancex in enumerate(explained_variance):\n",
    "    print(f\"Komponent główny {i+1}: Wyjaśniona wariancja = {explained_variancex:.4f}, Kumulatywna wyjaśniona wariancja = {cumulative_explained_variance[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame reduced_df zawiera cechy z analizy PCA przypisane do id filmu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
      "movieId                                                                         \n",
      "1        0.796514  2.212839 -2.489444  1.624747  0.984763  1.168046 -0.203300   \n",
      "2       -1.541922  1.597864 -1.113613  1.302526 -0.196336  0.191809 -0.026161   \n",
      "3       -1.506092 -0.010220 -0.605981 -0.316252  0.330972  0.334079  0.048253   \n",
      "4       -1.293604 -0.775662 -0.926582 -0.384600  0.103713 -0.345682  0.683775   \n",
      "5       -1.694557 -0.185986 -1.204239 -0.149549  0.470713  0.053199  0.385026   \n",
      "...           ...       ...       ...       ...       ...       ...       ...   \n",
      "205072  -0.034354  1.189431 -0.941385  0.439289  1.349124  0.065643 -0.668149   \n",
      "205076  -0.294929 -1.243930 -0.833182 -0.320838  0.265652  0.273416 -0.106014   \n",
      "205383   0.569254  0.952059  0.161266 -0.542846  0.214419  0.119859 -0.036601   \n",
      "205425   1.551432  1.517558  0.109042 -0.184095  1.932083  0.367802 -0.280007   \n",
      "206499   0.212250  1.392884  0.010167 -0.178252  1.152876 -0.113637  0.539744   \n",
      "\n",
      "              PC8       PC9      PC10  ...      PC41      PC42      PC43  \\\n",
      "movieId                                ...                                 \n",
      "1        1.659382 -0.675962 -1.251265  ...  0.065031  0.184335  0.064298   \n",
      "2        1.208987 -0.369388 -0.483199  ... -0.092240 -0.038333  0.394124   \n",
      "3        0.010072 -0.042318  0.040345  ...  0.284845 -0.018031 -0.270590   \n",
      "4        0.218370 -0.159426  0.639183  ...  0.258647 -0.022402  0.080714   \n",
      "5        0.302557 -0.133082 -0.083749  ...  0.377912  0.207129 -0.200088   \n",
      "...           ...       ...       ...  ...       ...       ...       ...   \n",
      "205072   0.739266  0.359374 -0.235279  ... -0.173309 -0.222389  0.158628   \n",
      "205076   0.179974  0.298846 -0.161174  ... -0.163591  0.107021 -0.120767   \n",
      "205383  -0.183716  0.082651 -0.177052  ... -0.227834 -0.041608 -0.174424   \n",
      "205425  -0.180244  0.470646 -0.494713  ... -0.011338  0.137657  0.092925   \n",
      "206499  -0.755982  0.464159  0.236271  ... -0.050730 -0.105908  0.128814   \n",
      "\n",
      "             PC44      PC45      PC46      PC47      PC48      PC49      PC50  \n",
      "movieId                                                                        \n",
      "1        0.098523  0.372167 -0.039389 -0.033540  0.087070  0.059638 -0.224841  \n",
      "2       -0.059242 -0.012435  0.007115  0.019579  0.019224 -0.281755  0.221073  \n",
      "3        0.104467 -0.172064  0.097237 -0.040866 -0.187794 -0.268885 -0.101578  \n",
      "4       -0.254559 -0.001771  0.030204  0.141675 -0.316801  0.187319 -0.034426  \n",
      "5       -0.046198 -0.168857  0.096808  0.278361 -0.054121 -0.125981 -0.155512  \n",
      "...           ...       ...       ...       ...       ...       ...       ...  \n",
      "205072  -0.371537  0.008751  0.538477 -0.063811 -0.255769  0.524418 -0.038642  \n",
      "205076  -0.138322  0.091829 -0.195301 -0.004187  0.047911  0.369714  0.064261  \n",
      "205383  -0.010719 -0.125438  0.199373 -0.040924 -0.086827 -0.095335  0.122315  \n",
      "205425   0.158208  0.171604  0.421586 -0.154436 -0.225336 -0.065872  0.013481  \n",
      "206499   0.103140  0.052752  0.194541 -0.092161  0.151520  0.057133  0.060952  \n",
      "\n",
      "[13816 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "reduced_features = pca.transform(data)\n",
    "reduced_df = pd.DataFrame(reduced_features, index=data.index, columns=[f\"PC{i}\" for i in range(1, n_components + 1)])\n",
    "print(reduced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scalenie danych dotyczacacy ocen z tymi dotyczacymi treści.**\n",
    "\n",
    "Postanowiłem usunąć dane dotyczące daty, ponieważ ta cecha wydaje się znancząca głównie w wypadku aktualnych danych, aby zauwazyć współczesne trendy, a dane które posiadam są juz dosyć stare. Jeśli chodzi o wykorzystanie sekwencji ogladania filmów, uznałem że nie jest to szczególnie ważny czynnik przy takim problemie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat = rat.drop(\"timestamp\", axis=1)\n",
    "merged_table = rat.merge(reduced_df, on='movieId', how='left')\n",
    "merged_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zauważyłem ze dużo uzytkowników nie ocenia \"dobrze\" nawet 10 filmów, dlatego postanowiłem ze mniej filmów będzie reprezentować ich preferencje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "high_ratings = merged_table[merged_table['rating'] >= 3.5]\n",
    "high_rating_counts = high_ratings.groupby('userId')['movieId'].count()\n",
    "few_high_rating_users = high_rating_counts[high_rating_counts < 8].index\n",
    "print(f\"Użytkownicy z mniej niż 8 dobrze ocenionymi filmami: {few_high_rating_users}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(high_rating_counts.info)\n",
    "print(merged_table.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Użytkowników którzy nie ocenili nawet 8 filmów dobrze postanowiłem usunąć, nie stanowią oni znaczącej liczby na tle wszystkich użytkowników, ale ich preferencje nie byłyby dobrze reprezentowane - filmy które wciąż ocenili słabo byłyby uznane za te które im się podobały"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_table = merged_table[~merged_table['userId'].isin(few_high_rating_users)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_table.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz tworzę dataframe któy zawiera po 20 cech 8 ulubionych filmów użytkownika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grupowanie danych według userId\n",
    "grouped = merged_table.groupby('userId')\n",
    "top_movies = pd.DataFrame()\n",
    "\n",
    "# Iterowanie po każdej użytkowniku\n",
    "for user_id, group in grouped:\n",
    "    # Wybór 5 najwyżej ocenionych filmów\n",
    "    top_8 = group.nlargest(8, 'rating')[[f'PC{i}' for i in range(1, 21)]]\n",
    "    top_8['userId'] = user_id\n",
    "    top_8.set_index('userId', inplace=True)\n",
    "    top_movies = pd.concat([top_movies, top_8])\n",
    "top_movies.to_csv('my_dataframe50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_movies = pd.read_csv(\"my_dataframe50.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 8 # number of rows to merge\n",
    "arr = top_movies.to_numpy().reshape(-1, n * top_movies.shape[1])\n",
    "new_df = pd.DataFrame(arr, index=top_movies.index.unique())\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('my_dataframe50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "new_dfw = pd.read_csv(\"my_dataframe50.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_dfw.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dfw=new_dfw.astype('float32')\n",
    "merged_table=merged_table.astype('float32')\n",
    "result = pd.merge(merged_table, new_dfw, on='userId')\n",
    "\n",
    "# Wyświetlenie wynikowego DataFrame\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(result.info())\n",
    "\n",
    "print(result.isnull().any(axis=1).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del merged_table, new_dfw , rat, data,  gs,reduced_df, reduced_features,movie,tag,gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=result.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = result['rating']\n",
    "X = result.drop(['userId', 'movieId', 'rating'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X,y,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_1, vector_2=X_train.iloc[:, :50], X_train.iloc[:, 50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdecydowałem się głownie na funkcje aktywacji Leaky Relu ze względu na to że sporo danych wejściowych to liczby ujemne, więc wolę uniknąć martwych neuronów.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, concatenate\n",
    "from keras.models import Model\n",
    "from keras.initializers import he_normal\n",
    "\n",
    "from keras.layers import LeakyReLU\n",
    "import numpy as np\n",
    "from keras.layers import Activation, Lambda\n",
    "\n",
    "# Definiujemy wejścia\n",
    "input_1 = Input(shape=(160,))\n",
    "input_2 = Input(shape=(50,))\n",
    "\n",
    "hidden_1 = Dense(160,kernel_initializer=he_normal())(input_1)\n",
    "hidden_1 = LeakyReLU(alpha=0.01)(hidden_1)\n",
    "hidden_11 = Dense(50,kernel_initializer=he_normal())(hidden_1)\n",
    "hidden_11 = LeakyReLU(alpha=0.01)(hidden_11)\n",
    "hidden_2 = Dense(50,kernel_initializer=he_normal())(input_2)\n",
    "hidden_2 = LeakyReLU(alpha=0.01)(hidden_2)\n",
    "\n",
    "# Konkatenacja warstw i warstwy, które mają wykryć zależności między\n",
    "concatenated = concatenate([hidden_11, hidden_2])\n",
    "hidden_ac = Dense(100,kernel_initializer=he_normal())(concatenated)\n",
    "hidden_ac = LeakyReLU(alpha=0.01)(hidden_ac)\n",
    "hidden_ac1 = Dense(100,kernel_initializer=he_normal())(hidden_ac)\n",
    "hidden_ac1 = LeakyReLU(alpha=0.01)(hidden_ac1)\n",
    "hidden_ac2 = Dense(100,kernel_initializer=he_normal())(hidden_ac1)\n",
    "hidden_ac2 = LeakyReLU(alpha=0.01)(hidden_ac2)\n",
    "hidden_ac3 = Dense(100,kernel_initializer=he_normal())(hidden_ac2)\n",
    "hidden_ac3 = LeakyReLU(alpha=0.01)(hidden_ac3)\n",
    "# Warstwa wyjściowa\n",
    "output = Dense(1)(hidden_ac3)\n",
    "output = Activation('sigmoid')(output)\n",
    "output = Lambda(lambda x: x * 5)(output)\n",
    "# Tworzenie modelu\n",
    "model = Model(inputs=[input_1, input_2], outputs=output)\n",
    "\n",
    "# Kompilacja modelu\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.info(), y_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_1.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector_1.isnull().any(axis=1).sum(), vector_2.isnull().any(axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model1=load_model(\"recommendation_model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping,TensorBoard\n",
    "\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\"recommendation_model2.h5\", save_best_only=True)\n",
    "early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "tensorboard = TensorBoard(log_dir=\"logs\")\n",
    "\n",
    "\n",
    "\n",
    "model1.fit([vector_2,vector_1], y_train, epochs=50, batch_size=32, validation_split=0.1, callbacks=[model_checkpoint, early_stopping, tensorboard])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "vector_1t = X_test.iloc[:, :50]\n",
    "vector_2t = X_test.iloc[:, 50:]\n",
    "model1=load_model(\"recommendation_model1.h5\")\n",
    "\n",
    "model1.evaluate([vector_2t,vector_1t], y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model1=load_model(\"recommendation_model1.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rows = reduced_df.loc[[168492,31148,192383,168492,31148,192383,168492,31148], ['PC' + str(i) for i in range(1, 21)]]\n",
    "vector_usera=rows.stack().reset_index(drop=True)\n",
    "input1pr = np.array(vector_usera).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_movie=reduced_df.loc[[858]]\n",
    "input2pr = np.array(vector_movie).reshape(1, -1)\n",
    "predictions = model.predict([input1pr,input2pr])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxim=1\n",
    "the_bests=1\n",
    "ff={}\n",
    "selected_indexes = reduced_df.index[(reduced_df.index >= 1) & (reduced_df.index < 209171)]\n",
    "for i in selected_indexes:\n",
    "    vector_movie=reduced_df.loc[[i]]\n",
    "    input2pr = np.array(vector_movie).reshape(1, -1)\n",
    "    predictions = model1.predict([input1pr,input2pr])\n",
    "    print(i)\n",
    "    print(\"bestie\",the_bests, maxim)\n",
    "    if predictions>4.0:\n",
    "        ff[i]=predictions\n",
    "    if predictions>maxim:\n",
    "        the_bests=i\n",
    "        maxim=predictions\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "film_data = {}\n",
    "\n",
    "for movie_id, prediction in ff.items():\n",
    "    row_index = movie[movie['movieId'] == movie_id].index[0]\n",
    "    \n",
    "    film_title = movie.loc[row_index, 'title']\n",
    "    \n",
    "    film_data[film_title] = prediction[0][0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = dict(sorted(film_data.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "print(sorted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "\n",
    "conn = sqlite3.connect('movies.db')\n",
    "\n",
    "\n",
    "columns = ', '.join(['PCA{} REAL'.format(i) for i in range(1, 51)])\n",
    "conn.execute('CREATE TABLE movies (movieId INT, title TEXT, {})'.format(columns))\n",
    "\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = {'PC{}'.format(i): 'PCA{}'.format(i) for i in range(1, 51)}\n",
    "reduced_df = reduced_df.rename(columns=column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "conn = sqlite3.connect('movies.db')\n",
    "columns = ['movieId'] + ['PCA{}'.format(i) for i in range(1, 51)]\n",
    "\n",
    "\n",
    "reduced_df.to_sql('movies', conn, if_exists='append', index=True)\n",
    "\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie=pd.read_csv(r\"ml-25m\\movies.csv\")\n",
    "movie = movie.loc[:, ['movieId', 'title']]\n",
    "\n",
    "conn = sqlite3.connect('movies.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "\n",
    "data = [(row['title'], row['movieId']) for _, row in movie.iterrows()]\n",
    "\n",
    "c.executemany('UPDATE movies SET title = ? WHERE movieId = ?', data)\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
